<h1>RML OSS BENCHMARKING</h1>
<h2>About</h2>
  This article aims at benchmarking 2 common machine learning libraries for a
  few machine learning algorithms. The target of this study is multiclass
  classification with sparse, not so sparse to dense datasets. We have explored
  various dataset for this work with the cardinality of sample space ranging from
  a few hundred to millions similarly for the cardinality of the feature space.
  The algorithms studied are : 
  <ul>
    <li>Logistic Regression.</li>
    <li>Softmax Regression.</li>
  </ul>

  The Frameworks explored are:
  <ul>
    <li>Mxnet.</li>
    <li>Sci-Kit Learn.</li>
  </ul>

  These linear algorithms are optimized with different optimizers, we have chosen linear
  classifiers because they have a fairly low training time. Mxnet which envisage to be a
  faster solution for training deep neural network doesnâ€™t support conventional
  optimization techniques rather it sticks to gradient descent and its variants popularly
  found in the deep learning literature. Whereas libraries like sci-kit learn which envisage
  to cater to the needs of machine learning in general supports various optimization
  techniques. Faster optimization techniques give sci-kit learn an advantage which
  stochastic gradient descent fails to give to mxnet although implementation of mxnet
  is much efficient.

<h2>Dataset Description</h2>
<h2>Experiment Description</h2>
  Logistic regression/softmax regression is trained in mxnet and sklearn to the same
  tolerance value and a comparison between the training time in both the framework is made.
  A logistic regression in mxnet is as a neuron with sigmoid loss obtained by putting LogisticRegressionOutput function in mxnet whereas for logistic regression in sklearn we have used the inbuilt LogisticRegresion class with multi_class set to ovr.
  A softmax regression in mxnet is a single layer neuron which has as many neurons as number of classes and has a softmax activation function whereas for softmax regression in sklearn we have used the inbuild LogisticRegression class with multi_class set to multinomial.
  Training time is recorded in both frameworks. Moreover, the logistic
  regression and softmax regression are optimized with newton-cg and lbfgs in sklearn
  whereas it is optimized with stochastic gradient descent and its variants in mxnet. The
  models are trained with 70% of the entire dataset and are tested on the remaining 30%
  of the data. Tolerance measure for our experiment is infinity gradient. The models are
  trained with cross-entropy loss which ensures convexity.
<h2>Setup</h2>
  The experiments have been carried out in google cloud instance with 8 CPU cores and
  50 GB RAM size. The size of RAM was kept high so that swap time can be prevented.
<h2>Experimentaton and Results</h2>
  The datasets were normalized before training. Moreover, the models were zero-initialized 
  so that a model in different framework has the same initial starting point.
  The models are trained until convergence and are check
  pointed at various points so that a comparison between
  frameworks are possible. This is done by obtaining the tolerance measure(which was
  the inifinity norm of gradient after 100,200,400,800,1600... until convergence) and time taken
  for gradient descent in mxnet and time taken by Netwton-cg and lbfgs in sklearn to obtain the same tolerance. 
  Hence a comparison of time taken between different frameworks to obtain the same tolerance measure was done.
  The obtained results are summarized in the form of graphs shown below.<br>
<h3>Logistic Regression Results</h3>
 <figure>
 <img src="/rml-oss-benchmark/logistic_plots/svm_guide_1.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>SVM Guide 1 with 3089 samples and 4 features</figcaption>
  <img src="/rml-oss-benchmark/logistic_plots/phishing.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Phishing with 11055 samples and 30 features</figcaption>
  <img src="/rml-oss-benchmark/logistic_plots/madelon.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Madelon with 200 samples and 500 features</figcaption>
  
   <img src="/rml-oss-benchmark/logistic_plots/cod_rna.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Cod_RNA with 59535 samples and 8 features</figcaption>
  <img src="/rml-oss-benchmark/logistic_plots/w9a.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>W9a with 32561 samples and 123 features</figcaption>
   <img src="/rml-oss-benchmark/logistic_plots/real_sim.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Real Sim with 72309 samples and 20958 features</figcaption>
  
   <img src="/rml-oss-benchmark/logistic_plots/rcv_1.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>RCV1 with 20242 samples and 	47236 features</figcaption>
  <img src="/rml-oss-benchmark/logistic_plots/news20.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>News 20 with 19996 samples and 1355191 features</figcaption>
  <img src="/rml-oss-benchmark/logistic_plots/sussy.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Sussy with 5000000 samples and 18 features</figcaption>
  
</figure>
<h3>Softmax  Regression Results</h3>
<figure>
  <img src="/rml-oss-benchmark/softmax_plots/iris.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Iris with 150 samples, 4 features and 3 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/segment.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Segment with 2310 samples, 19 features and 7 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/satimage.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Segment with 4435 samples, 36 features and 6 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/pendigits_1.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Pendigits with 7494 samples, 16 features and 10 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/dna.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>DNA with 2000 samples, 180 features and 3 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/usps.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>USPS with 7291 samples, 16 features and 10 classes</figcaption>
  
   <img src="/rml-oss-benchmark/softmax_plots/letter.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Letter with 15000 samples, 256 features and 26 classes</figcaption>
   <img src="/rml-oss-benchmark/softmax_plots/sensorless.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Sensorless with 58509 samples, 48 features and 11 classes</figcaption>
  <img src="/rml-oss-benchmark/softmax_plots/mnist_1.png" alt="hi" height="500" width="1000" class="inline"/>
  <figcaption>Mnist with 60000 samples, 780 features and 10 classes</figcaption>
</figure>
<h2>Conclusion</h2>
<h3>Logistic Regression</h3>
  When the cardinality of the feature space is less a logistic regression trained in mxnet
  using sgd converges faster in comparison with a logistic regression trained in sklearn
  optimized using lbfgs and newton-cg. Which can be attributed to better
  implementation of the framework whereas the same machine learning model takes
  comparable time when the feature space is larger and is sparse.
<h3>Softmax Regression </h3>
  When the cardinality of the feature space is less a softmax regression trained in mxnet
  with sgd converges much later in comparison with a softmax regression trained in
  sklearn optimized using lbfgs and newton-cg. When the cardinality of the feature space
  is sparse the same model trained in sklearn optimized with newton-cg or lbfgs is order
  of magnitude faster than the model trained in mxnet optimized with sgd and its
  variants.
